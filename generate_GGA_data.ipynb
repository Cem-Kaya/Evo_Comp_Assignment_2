{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph: Graph500.txt\n",
      "Max Iterations: 1000\n",
      "Population Sizes: [100]\n",
      "Number of Runs per Pop Size: 3\n",
      "Base Random Seed: 4288\n",
      "Output Directory: ./pckl/GGLS/\n",
      "------------------------------------------\n",
      "--- Processing Population Size = 100 ---\n",
      "  Running Run 1/3 (Seed: 4389)...\n",
      "    Completed in 4845.09s. Best Cut: 5\n",
      "  Running Run 2/3 (Seed: 4390)...\n",
      "    Completed in 4462.77s. Best Cut: 2\n",
      "  Running Run 3/3 (Seed: 4391)...\n",
      "    Completed in 3966.37s. Best Cut: 8\n",
      "--- Finished all 3 runs for Population Size = 100 ---\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime\n",
    "from gls_gen import GLS_GEN\n",
    "\n",
    "GRAPH_FILENAME = \"Graph500.txt\"\n",
    "MAX_ITERATIONS = 1_000 # 256 \n",
    "RANDOM_SEED_BASE = 4288 # seed for the seed, will be modified for each run\n",
    "NUM_RUNS = 3\n",
    "POP_SIZES = [ 100,] \n",
    "OUTPUT_DIR = './pckl/GGLS/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Graph: {GRAPH_FILENAME}\")\n",
    "print(f\"Max Iterations: {MAX_ITERATIONS}\")\n",
    "print(f\"Population Sizes: {POP_SIZES}\")\n",
    "print(f\"Number of Runs per Pop Size: {NUM_RUNS}\")\n",
    "print(f\"Base Random Seed: {RANDOM_SEED_BASE}\")\n",
    "print(f\"Output Directory: {OUTPUT_DIR}\")\n",
    "print(\"-\" * 42) # Your separator length\n",
    "\n",
    "# Loop through each population size\n",
    "for pop_size in POP_SIZES:\n",
    "    print(f\"--- Processing Population Size = {pop_size} ---\")\n",
    "    results_for_this_pop_size = []\n",
    "\n",
    "    for run_num in range(1, NUM_RUNS + 1):\n",
    "        # Generate a unique seed\n",
    "        current_seed = RANDOM_SEED_BASE + pop_size + run_num\n",
    "\n",
    "        print(f\"  Running Run {run_num}/{NUM_RUNS} (Seed: {current_seed})...\")\n",
    "\n",
    "        \n",
    "        gls_runner = GLS_GEN(\n",
    "            graph_filename=GRAPH_FILENAME,\n",
    "            pop_size=pop_size,\n",
    "            max_iterations=MAX_ITERATIONS,\n",
    "            random_seed=current_seed\n",
    "        )\n",
    "\n",
    "        # Run \n",
    "        run_start_time = time.time()\n",
    "        best_cut_found = gls_runner.run_gls()\n",
    "        run_end_time = time.time()\n",
    "        print(f\"    Completed in {run_end_time - run_start_time:.2f}s. Best Cut: {best_cut_found}\")\n",
    "\n",
    "        # Get  statistics\n",
    "        run_stats = gls_runner.get_run_statistics()\n",
    "\n",
    "        # Add run data  \n",
    "        run_stats['run_number'] = run_num\n",
    "        run_stats['run_seed'] = current_seed\n",
    "        # Add a timestamp for this specific \n",
    "        run_stats['timestamp'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        results_for_this_pop_size.append(run_stats)\n",
    "\n",
    "\n",
    "    consolidated_timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    \n",
    "    output_filename = f\"gls_pop{pop_size}_iter_count_{MAX_ITERATIONS}_runs{NUM_RUNS}_{consolidated_timestamp_str}.pkl\"\n",
    "    output_filepath = os.path.join(OUTPUT_DIR, output_filename)\n",
    "\n",
    "    \n",
    "    with open(output_filepath, 'wb') as f_out:\n",
    "        pickle.dump(results_for_this_pop_size, f_out) \n",
    "\n",
    "    print(f\"--- Finished all {NUM_RUNS} runs for Population Size = {pop_size} ---\")\n",
    "    print(\"-\" * 42) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning directory: w:\\Google_drive\\Sync\\UU\\1_3\\Evo\\HW\\2\\Evo_Comp_Assignment_2\\pckl\\GGLS\n",
      "Looking for Pop Sizes: [100, 10, 20, 50, 6] with Iter Count: 1000\n",
      "Merged files will be saved to: w:\\Google_drive\\Sync\\UU\\1_3\\Evo\\HW\\2\\Evo_Comp_Assignment_2\\pckl\\merged\n",
      "----------------------------------------\n",
      "  Found for Pop 100: gls_pop100_iter_count_1000_runs3_20250331_225146.pkl\n",
      "  Found for Pop 100: gls_pop100_iter_count_1000_runs3_20250401_160110.pkl\n",
      "  Found for Pop 100: gls_pop100_iter_count_1000_runs4_20250401_170402.pkl\n",
      "  Found for Pop 10: gls_pop10_iter_count_1000_runs3_20250331_170234.pkl\n",
      "  Found for Pop 10: gls_pop10_iter_count_1000_runs7_20250331_191112.pkl\n",
      "  Found for Pop 20: gls_pop20_iter_count_1000_runs3_20250331_174635.pkl\n",
      "  Found for Pop 20: gls_pop20_iter_count_1000_runs7_20250331_204756.pkl\n",
      "  Found for Pop 50: gls_pop50_iter_count_1000_runs3_20250331_193116.pkl\n",
      "  Found for Pop 50: gls_pop50_iter_count_1000_runs7_20250401_003758.pkl\n",
      "  Found for Pop 6: gls_pop6_iter_count_1000_runs3_20250331_164303.pkl\n",
      "  Found for Pop 6: gls_pop6_iter_count_1000_runs7_20250331_182349.pkl\n",
      "\n",
      "--- Grouping Complete ---\n",
      "Group Pop 100: 3 file(s)\n",
      "Group Pop 10: 2 file(s)\n",
      "Group Pop 20: 2 file(s)\n",
      "Group Pop 50: 2 file(s)\n",
      "Group Pop 6: 2 file(s)\n",
      "\n",
      "--- Merging Files ---\n",
      "Processing group: Population Size = 100, Iterations = 1000\n",
      "  Loading data from: gls_pop100_iter_count_1000_runs3_20250331_225146.pkl\n",
      "    -> Loaded 3 run results.\n",
      "  Loading data from: gls_pop100_iter_count_1000_runs3_20250401_160110.pkl\n",
      "    -> Loaded 3 run results.\n",
      "  Loading data from: gls_pop100_iter_count_1000_runs4_20250401_170402.pkl\n",
      "    -> Loaded 4 run results.\n",
      "  Saving combined data (10 total runs) to: gls_pop100_iter_count_1000_runs10_merged_20250401_195612.pkl (in ./pckl/merged/)\n",
      "  Successfully saved merged file.\n",
      "--------------------\n",
      "Processing group: Population Size = 10, Iterations = 1000\n",
      "  Loading data from: gls_pop10_iter_count_1000_runs3_20250331_170234.pkl\n",
      "    -> Loaded 3 run results.\n",
      "  Loading data from: gls_pop10_iter_count_1000_runs7_20250331_191112.pkl\n",
      "    -> Loaded 7 run results.\n",
      "  Saving combined data (10 total runs) to: gls_pop10_iter_count_1000_runs10_merged_20250401_195612.pkl (in ./pckl/merged/)\n",
      "  Successfully saved merged file.\n",
      "--------------------\n",
      "Processing group: Population Size = 20, Iterations = 1000\n",
      "  Loading data from: gls_pop20_iter_count_1000_runs3_20250331_174635.pkl\n",
      "    -> Loaded 3 run results.\n",
      "  Loading data from: gls_pop20_iter_count_1000_runs7_20250331_204756.pkl\n",
      "    -> Loaded 7 run results.\n",
      "  Saving combined data (10 total runs) to: gls_pop20_iter_count_1000_runs10_merged_20250401_195612.pkl (in ./pckl/merged/)\n",
      "  Successfully saved merged file.\n",
      "--------------------\n",
      "Processing group: Population Size = 50, Iterations = 1000\n",
      "  Loading data from: gls_pop50_iter_count_1000_runs3_20250331_193116.pkl\n",
      "    -> Loaded 3 run results.\n",
      "  Loading data from: gls_pop50_iter_count_1000_runs7_20250401_003758.pkl\n",
      "    -> Loaded 7 run results.\n",
      "  Saving combined data (10 total runs) to: gls_pop50_iter_count_1000_runs10_merged_20250401_195613.pkl (in ./pckl/merged/)\n",
      "  Successfully saved merged file.\n",
      "--------------------\n",
      "Processing group: Population Size = 6, Iterations = 1000\n",
      "  Loading data from: gls_pop6_iter_count_1000_runs3_20250331_164303.pkl\n",
      "    -> Loaded 3 run results.\n",
      "  Loading data from: gls_pop6_iter_count_1000_runs7_20250331_182349.pkl\n",
      "    -> Loaded 7 run results.\n",
      "  Saving combined data (10 total runs) to: gls_pop6_iter_count_1000_runs10_merged_20250401_195613.pkl (in ./pckl/merged/)\n",
      "  Successfully saved merged file.\n",
      "--------------------\n",
      "\n",
      "--- Merging Process Finished ---\n",
      "Merged files should be in: w:\\Google_drive\\Sync\\UU\\1_3\\Evo\\HW\\2\\Evo_Comp_Assignment_2\\pckl\\merged\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Configuration ---\n",
    "# Directory where the original pickle files are located,\n",
    "# relative to where you run the script.\n",
    "INPUT_DIR = './pckl/GGLS/' \n",
    "\n",
    "# Directory where the merged pickle files will be saved,\n",
    "# relative to where you run the script.\n",
    "OUTPUT_DIR_MERGED = './pckl/merged/' \n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "# This will correctly create w:\\...\\Evo_Comp_Assignment_2\\pckl\\merged\\\n",
    "os.makedirs(OUTPUT_DIR_MERGED, exist_ok=True)\n",
    "\n",
    "# Iteration count is constant in your example\n",
    "ITERATION_COUNT = 1000 \n",
    "\n",
    "# Population sizes identified from your file list\n",
    "POP_SIZES_TO_PROCESS = [100, 10, 20, 50, 6] \n",
    "\n",
    "# --- Logic ---\n",
    "\n",
    "print(f\"Scanning directory: {os.path.abspath(INPUT_DIR)}\") # Show absolute path being scanned\n",
    "print(f\"Looking for Pop Sizes: {POP_SIZES_TO_PROCESS} with Iter Count: {ITERATION_COUNT}\")\n",
    "print(f\"Merged files will be saved to: {os.path.abspath(OUTPUT_DIR_MERGED)}\") # Show absolute path for output\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Check if INPUT_DIR exists before trying to list its contents\n",
    "if not os.path.isdir(INPUT_DIR):\n",
    "    print(f\"Error: Input directory not found at {os.path.abspath(INPUT_DIR)}\")\n",
    "    print(\"Please ensure the script is run from the correct parent directory ('Evo_Comp_Assignment_2')\")\n",
    "    exit() # Stop the script if the input directory isn't found\n",
    "\n",
    "all_files_in_dir = [f for f in os.listdir(INPUT_DIR) if f.endswith(\".pkl\")]\n",
    "\n",
    "if not all_files_in_dir:\n",
    "    print(f\"Warning: No .pkl files found in {os.path.abspath(INPUT_DIR)}\")\n",
    "\n",
    "# Group files manually based on known pop sizes\n",
    "grouped_files = defaultdict(list)\n",
    "\n",
    "for pop_size in POP_SIZES_TO_PROCESS:\n",
    "    # Construct the expected prefix for filenames of this group\n",
    "    file_prefix = f\"gls_pop{pop_size}_iter_count_{ITERATION_COUNT}_\"\n",
    "    \n",
    "    for filename in all_files_in_dir:\n",
    "        if filename.startswith(file_prefix):\n",
    "            # Store the full path relative to the script's execution directory\n",
    "            grouped_files[pop_size].append(os.path.join(INPUT_DIR, filename)) \n",
    "            print(f\"  Found for Pop {pop_size}: {filename}\") # Print just the filename for clarity\n",
    "\n",
    "print(\"\\n--- Grouping Complete ---\")\n",
    "for pop_size, files in grouped_files.items():\n",
    "    if files:\n",
    "        print(f\"Group Pop {pop_size}: {len(files)} file(s)\")\n",
    "        # for f in files:\n",
    "        #    print(f\"    - {f}\") # Print full path if needed for debugging\n",
    "    else:\n",
    "        print(f\"Group Pop {pop_size}: No files found matching prefix.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Merging Files ---\")\n",
    "\n",
    "# Iterate through each group and merge the data\n",
    "for pop_size, filepaths in grouped_files.items(): # Note: Renamed 'filenames' to 'filepaths' for clarity\n",
    "    if len(filepaths) < 1: \n",
    "        print(f\"Skipping group Pop {pop_size}: No files listed.\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"Processing group: Population Size = {pop_size}, Iterations = {ITERATION_COUNT}\")\n",
    "    \n",
    "    combined_results = []\n",
    "    total_runs_loaded = 0\n",
    "\n",
    "    # Now iterate through the full filepaths\n",
    "    for filepath in filepaths: \n",
    "        # Extract just the filename for printing, if desired\n",
    "        filename_only = os.path.basename(filepath) \n",
    "        print(f\"  Loading data from: {filename_only}\") \n",
    "        try:\n",
    "            # Open using the full filepath\n",
    "            with open(filepath, 'rb') as f_in: \n",
    "                data = pickle.load(f_in)\n",
    "                if isinstance(data, list):\n",
    "                    combined_results.extend(data)\n",
    "                    total_runs_loaded += len(data) \n",
    "                    print(f\"    -> Loaded {len(data)} run results.\")\n",
    "                else:\n",
    "                    print(f\"    -> Warning: Expected a list but got {type(data)}. Skipping file content.\")\n",
    "        except FileNotFoundError:\n",
    "            # This shouldn't happen if os.listdir worked, but good to keep\n",
    "            print(f\"    -> Error: File not found: {filepath}\") \n",
    "        except pickle.UnpicklingError as e:\n",
    "            print(f\"    -> Error: Could not unpickle file {filepath}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"    -> Error: An unexpected error occurred loading {filepath}: {e}\")\n",
    "\n",
    "    if not combined_results:\n",
    "        print(f\"  No data successfully loaded for group Pop {pop_size}. Skipping merge.\")\n",
    "        continue\n",
    "\n",
    "    # Save the combined data to a new pickle file in the correct output directory\n",
    "    merged_timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_filename = f\"gls_pop{pop_size}_iter_count_{ITERATION_COUNT}_runs{total_runs_loaded}_merged_{merged_timestamp_str}.pkl\"\n",
    "    # Construct the full output path\n",
    "    output_filepath = os.path.join(OUTPUT_DIR_MERGED, output_filename) \n",
    "\n",
    "    print(f\"  Saving combined data ({total_runs_loaded} total runs) to: {output_filename} (in {OUTPUT_DIR_MERGED})\")\n",
    "    try:\n",
    "        with open(output_filepath, 'wb') as f_out:\n",
    "            pickle.dump(combined_results, f_out)\n",
    "        print(f\"  Successfully saved merged file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: Could not save merged file {output_filepath}: {e}\")\n",
    "        \n",
    "    print(\"-\" * 20) \n",
    "\n",
    "print(\"\\n--- Merging Process Finished ---\")\n",
    "print(f\"Merged files should be in: {os.path.abspath(OUTPUT_DIR_MERGED)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
